From 9558b6375e4d57a8bff7a2e38991592258a0f5f0 Mon Sep 17 00:00:00 2001
From: Lu Baolu <baolu.lu@linux.intel.com>
Date: Thu, 19 May 2022 09:37:31 +0800
Subject: [PATCH 097/141] iommu/vt-d: Add support for nested mode translation

It includes:

- Allocate and free nested domains;
- Cache invalidation for nested domains;
- Setup/teardown nested translation structures for transfers w/o pasid.
- Setup/teardown nested translation structures for transfers w/ pasid.

Signed-off-by: Lu Baolu <baolu.lu@linux.intel.com>

1	1	drivers/iommu/intel/Makefile
2	0	drivers/iommu/intel/iommu.c
290	0	drivers/iommu/intel/nested.c
17	0	drivers/iommu/intel/nested.h
135	0	drivers/iommu/intel/pasid.c
5	0	drivers/iommu/intel/pasid.h
1	0	include/linux/intel-iommu.h

diff --git a/drivers/iommu/intel/Makefile b/drivers/iommu/intel/Makefile
index fa0dae16441c..8b324db03692 100644
--- a/drivers/iommu/intel/Makefile
+++ b/drivers/iommu/intel/Makefile
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0
 obj-$(CONFIG_DMAR_TABLE) += dmar.o
-obj-$(CONFIG_INTEL_IOMMU) += iommu.o pasid.o
+obj-$(CONFIG_INTEL_IOMMU) += iommu.o pasid.o nested.o
 obj-$(CONFIG_DMAR_TABLE) += trace.o cap_audit.o
 obj-$(CONFIG_DMAR_PERF) += perf.o
 obj-$(CONFIG_INTEL_IOMMU_DEBUGFS) += debugfs.o
diff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c
index 2fb42dc99f2c..255754707856 100644
--- a/drivers/iommu/intel/iommu.c
+++ b/drivers/iommu/intel/iommu.c
@@ -30,6 +30,7 @@
 #include "../iommu-sva.h"
 #include "pasid.h"
 #include "cap_audit.h"
+#include "nested.h"
 
 #define ROOT_SIZE		VTD_PAGE_SIZE
 #define CONTEXT_SIZE		VTD_PAGE_SIZE
@@ -5047,6 +5048,7 @@ static void intel_iommu_detach_device_pasid(struct iommu_domain *domain,
 const struct iommu_ops intel_iommu_ops = {
 	.capable		= intel_iommu_capable,
 	.domain_alloc		= intel_iommu_domain_alloc,
+	.nested_domain_alloc	= intel_nested_domain_alloc,
 	.probe_device		= intel_iommu_probe_device,
 	.probe_finalize		= intel_iommu_probe_finalize,
 	.release_device		= intel_iommu_release_device,
diff --git a/drivers/iommu/intel/nested.c b/drivers/iommu/intel/nested.c
new file mode 100644
index 000000000000..95d7d1863ed3
--- /dev/null
+++ b/drivers/iommu/intel/nested.c
@@ -0,0 +1,290 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * nested.c - nested mode translation support
+ *
+ * Copyright (C) 2022 Intel Corporation
+ *
+ * Author: Lu Baolu <baolu.lu@linux.intel.com>
+ */
+
+#define pr_fmt(fmt)	"DMAR: " fmt
+
+#include <linux/intel-iommu.h>
+#include <linux/iommu.h>
+#include <linux/pci.h>
+#include <linux/pci-ats.h>
+
+#include "pasid.h"
+
+struct nested_domain {
+	struct dmar_domain *s2_domain;
+	unsigned long s1_pgtbl;
+	struct iommu_stage1_config_vtd *s1_cfg;
+
+	/* Protect the device list. */
+	struct mutex mutex;
+	struct list_head devices;
+
+	struct iommu_domain domain;
+};
+
+#define to_nested_domain(dom) container_of(dom, struct nested_domain, domain)
+
+static int intel_nested_attach_dev_pasid(struct iommu_domain *domain,
+					 struct device *dev, ioasid_t pasid)
+{
+	struct device_domain_info *info = dev_iommu_priv_get(dev);
+	struct nested_domain *ndomain = to_nested_domain(domain);
+	struct dmar_domain *dmar_domain = ndomain->s2_domain;
+	struct intel_iommu *iommu = info->iommu;
+	int ret = 0;
+
+	spin_lock(&iommu->lock);
+	ret = intel_pasid_setup_nested(iommu, dev, pasid,
+				       (pgd_t *)(uintptr_t)ndomain->s1_pgtbl,
+				       dmar_domain, ndomain->s1_cfg);
+	spin_unlock(&iommu->lock);
+	if (ret)
+		return ret;
+
+	mutex_lock(&ndomain->mutex);
+	list_add(&info->nested, &ndomain->devices);
+	mutex_unlock(&ndomain->mutex);
+
+	return ret;
+}
+
+static void intel_nested_detach_dev_pasid(struct iommu_domain *domain,
+					  struct device *dev, ioasid_t pasid)
+{
+	struct device_domain_info *info = dev_iommu_priv_get(dev);
+	struct nested_domain *ndomain = to_nested_domain(domain);
+	struct intel_iommu *iommu = info->iommu;
+
+	spin_lock(&iommu->lock);
+	intel_pasid_tear_down_entry(iommu, dev, pasid, false);
+	/* Revist: Need to drain the prq when PR is support. */
+	spin_unlock(&iommu->lock);
+
+	mutex_lock(&ndomain->mutex);
+	list_del(&info->nested);
+	mutex_unlock(&ndomain->mutex);
+}
+
+static int intel_nested_attach_dev(struct iommu_domain *domain,
+				   struct device *dev)
+{
+	return intel_nested_attach_dev_pasid(domain, dev, PASID_RID2PASID);
+}
+
+static void intel_nested_detach_dev(struct iommu_domain *domain,
+				    struct device *dev)
+{
+	intel_nested_detach_dev_pasid(domain, dev, PASID_RID2PASID);
+}
+
+static void intel_nested_domain_free(struct iommu_domain *domain)
+{
+	kfree(to_nested_domain(domain));
+}
+
+/*
+ * 2D array for converting and sanitizing IOMMU generic TLB granularity to
+ * VT-d granularity. Invalidation is typically included in the unmap operation
+ * as a result of DMA or VFIO unmap. However, for assigned devices guest
+ * owns the first level page tables. Invalidations of translation caches in the
+ * guest are trapped and passed down to the host.
+ *
+ * vIOMMU in the guest will only expose first level page tables, therefore
+ * we do not support IOTLB granularity for request without PASID (second level).
+ *
+ * For example, to find the VT-d granularity encoding for IOTLB
+ * type and page selective granularity within PASID:
+ * X: indexed by iommu cache type
+ * Y: indexed by enum iommu_inv_granularity
+ * [IOMMU_CACHE_INV_TYPE_IOTLB][IOMMU_INV_GRANU_ADDR]
+ */
+static const int
+inv_type_granu_table[IOMMU_CACHE_INV_TYPE_NR][IOMMU_INV_GRANU_NR] = {
+	/*
+	 * PASID based IOTLB invalidation: PASID selective (per PASID),
+	 * page selective (address granularity)
+	 */
+	{-EINVAL, QI_GRAN_NONG_PASID, QI_GRAN_PSI_PASID},
+	/* PASID based dev TLBs */
+	{-EINVAL, -EINVAL, QI_DEV_IOTLB_GRAN_PASID_SEL},
+	/* PASID cache */
+	{-EINVAL, -EINVAL, -EINVAL}
+};
+
+static inline int to_vtd_granularity(int type, int granu)
+{
+	return inv_type_granu_table[type][granu];
+}
+
+static inline u64 to_vtd_size(u64 granu_size, u64 nr_granules)
+{
+	u64 nr_pages = (granu_size * nr_granules) >> VTD_PAGE_SHIFT;
+
+	/* VT-d size is encoded as 2^size of 4K pages, 0 for 4k, 9 for 2MB, etc.
+	 * IOMMU cache invalidate API passes granu_size in bytes, and number of
+	 * granu size in contiguous memory.
+	 */
+	return order_base_2(nr_pages);
+}
+
+static int
+intel_nested_invalidate(struct dmar_domain *domain, struct device *dev,
+			struct iommu_cache_invalidate_info *inv_info)
+{
+	struct device_domain_info *info = dev_iommu_priv_get(dev);
+	struct intel_iommu *iommu = info->iommu;
+	u8 devfn = info->devfn;
+	u8 bus = info->bus;
+	int cache_type;
+	u16 did, sid;
+	int ret = 0;
+	u64 size = 0;
+
+	did = domain->iommu_did[iommu->seq_id];
+	sid = PCI_DEVID(bus, devfn);
+
+	/* Size is only valid in address selective invalidation */
+	if (inv_info->granularity == IOMMU_INV_GRANU_ADDR)
+		size = to_vtd_size(inv_info->granu.addr_info.granule_size,
+				   inv_info->granu.addr_info.nb_granules);
+
+	for_each_set_bit(cache_type,
+			 (unsigned long *)&inv_info->cache,
+			 IOMMU_CACHE_INV_TYPE_NR) {
+		int granu = 0;
+		u64 pasid = 0;
+		u64 addr = 0;
+
+		granu = to_vtd_granularity(cache_type, inv_info->granularity);
+		if (granu == -EINVAL) {
+			pr_err_ratelimited("Invalid cache type and granu combination %d/%d\n",
+					   cache_type, inv_info->granularity);
+			break;
+		}
+
+		/*
+		 * PASID is stored in different locations based on the
+		 * granularity.
+		 */
+		if (inv_info->granularity == IOMMU_INV_GRANU_PASID &&
+		    (inv_info->granu.pasid_info.flags & IOMMU_INV_PASID_FLAGS_PASID))
+			pasid = inv_info->granu.pasid_info.pasid;
+		else if (inv_info->granularity == IOMMU_INV_GRANU_ADDR &&
+			 (inv_info->granu.addr_info.flags & IOMMU_INV_ADDR_FLAGS_PASID))
+			pasid = inv_info->granu.addr_info.pasid;
+
+		switch (BIT(cache_type)) {
+		case IOMMU_CACHE_INV_TYPE_IOTLB:
+			/* HW will ignore LSB bits based on address mask */
+			if (inv_info->granularity == IOMMU_INV_GRANU_ADDR &&
+			    size &&
+			    (inv_info->granu.addr_info.addr & ((BIT(VTD_PAGE_SHIFT + size)) - 1))) {
+				pr_err_ratelimited("User address not aligned, 0x%llx, size order %llu\n",
+						   inv_info->granu.addr_info.addr, size);
+			}
+
+			/*
+			 * If granu is PASID-selective, address is ignored.
+			 * We use npages = -1 to indicate that.
+			 */
+			qi_flush_piotlb(iommu, did, pasid,
+					inv_info->granu.addr_info.addr,
+					(granu == QI_GRAN_NONG_PASID) ? -1 : 1 << size,
+					inv_info->granu.addr_info.flags & IOMMU_INV_ADDR_FLAGS_LEAF);
+
+			if (!info->ats_enabled)
+				break;
+			/*
+			 * Always flush device IOTLB if ATS is enabled. vIOMMU
+			 * in the guest may assume IOTLB flush is inclusive,
+			 * which is more efficient.
+			 */
+			fallthrough;
+		case IOMMU_CACHE_INV_TYPE_DEV_IOTLB:
+			/*
+			 * PASID based device TLB invalidation does not support
+			 * IOMMU_INV_GRANU_PASID granularity but only supports
+			 * IOMMU_INV_GRANU_ADDR.
+			 * The equivalent of that is we set the size to be the
+			 * entire range of 64 bit. User only provides PASID info
+			 * without address info. So we set addr to 0.
+			 */
+			if (inv_info->granularity == IOMMU_INV_GRANU_PASID) {
+				size = 64 - VTD_PAGE_SHIFT;
+				addr = 0;
+			} else if (inv_info->granularity == IOMMU_INV_GRANU_ADDR) {
+				addr = inv_info->granu.addr_info.addr;
+			}
+
+			if (info->ats_enabled)
+				qi_flush_dev_iotlb_pasid(iommu, sid,
+						info->pfsid, pasid,
+						info->ats_qdep, addr,
+						size);
+			else
+				pr_warn_ratelimited("Passdown device IOTLB flush w/o ATS!\n");
+			break;
+		default:
+			dev_err_ratelimited(dev, "Unsupported IOMMU invalidation type %d\n",
+					    cache_type);
+			ret = -EINVAL;
+		}
+	}
+
+	return ret;
+}
+
+static int
+intel_nested_cache_invalidate(struct iommu_domain *domain,
+			      struct iommu_cache_invalidate_info *inv_info)
+{
+	struct nested_domain *ndomain = to_nested_domain(domain);
+	struct device_domain_info *info;
+	int ret = 0;
+
+	mutex_lock(&ndomain->mutex);
+	list_for_each_entry(info, &ndomain->devices, nested) {
+		ret = intel_nested_invalidate(ndomain->s2_domain,
+					      info->dev, inv_info);
+		if (ret)
+			break;
+	}
+	mutex_unlock(&ndomain->mutex);
+
+	return ret;
+}
+
+static const struct iommu_domain_ops intel_nested_domain_ops = {
+	.attach_dev		= intel_nested_attach_dev,
+	.detach_dev		= intel_nested_detach_dev,
+	.attach_dev_pasid	= intel_nested_attach_dev_pasid,
+	.detach_dev_pasid	= intel_nested_detach_dev_pasid,
+	.cache_invalidate	= intel_nested_cache_invalidate,
+	.free			= intel_nested_domain_free,
+};
+
+struct iommu_domain *intel_nested_domain_alloc(struct iommu_domain *s2_domain,
+					       unsigned long s1_pgtbl,
+					       union iommu_stage1_config *cfg)
+{
+	struct nested_domain *ndomain;
+
+	ndomain = kzalloc(sizeof(*ndomain), GFP_KERNEL);
+	if (!ndomain)
+		return NULL;
+
+	ndomain->s2_domain = to_dmar_domain(s2_domain);
+	ndomain->s1_pgtbl = s1_pgtbl;
+	ndomain->s1_cfg = &cfg->vtd;
+	ndomain->domain.ops = &intel_nested_domain_ops;
+	mutex_init(&ndomain->mutex);
+	INIT_LIST_HEAD(&ndomain->devices);
+
+	return &ndomain->domain;
+}
diff --git a/drivers/iommu/intel/nested.h b/drivers/iommu/intel/nested.h
new file mode 100644
index 000000000000..9c6a61c8fb9e
--- /dev/null
+++ b/drivers/iommu/intel/nested.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2022 Intel Corporation
+ *
+ * Author: Lu Baolu <baolu.lu@linux.intel.com>
+ */
+
+#ifndef __INTEL_NESTED_H
+#define __INTEL_NESTED_H
+
+#include <uapi/linux/iommufd.h>
+
+struct iommu_domain *intel_nested_domain_alloc(struct iommu_domain *s2_domain,
+					       unsigned long s1_ptr,
+					       union iommu_stage1_config *cfg);
+
+#endif /* __INTEL_NESTED_H */
diff --git a/drivers/iommu/intel/pasid.c b/drivers/iommu/intel/pasid.c
index f8d215d85695..15a4cf72d3ac 100644
--- a/drivers/iommu/intel/pasid.c
+++ b/drivers/iommu/intel/pasid.c
@@ -762,3 +762,138 @@ int intel_pasid_setup_pass_through(struct intel_iommu *iommu,
 
 	return 0;
 }
+
+/**
+ * intel_pasid_setup_nested() - Set up PASID entry for nested translation.
+ * This could be used for guest shared virtual address. In this case, the
+ * first level page tables are used for GVA-GPA translation in the guest,
+ * second level page tables are used for GPA-HPA translation.
+ *
+ * @iommu:      IOMMU which the device belong to
+ * @dev:        Device to be set up for translation
+ * @pasid:      PASID to be programmed in the device PASID table
+ * @s1_gpgd:    FLPTPTR: First Level Page translation pointer in GPA
+ * @s2_domain:  Domain info for setting up second level page tables
+ * @s1_cfg:     Additional PASID info from the guest bind request
+ */
+int intel_pasid_setup_nested(struct intel_iommu *iommu, struct device *dev,
+			     u32 pasid, pgd_t *s1_gpgd,
+			     struct dmar_domain *s2_domain,
+			     struct iommu_stage1_config_vtd *s1_cfg)
+{
+	struct pasid_entry *pte;
+	struct dma_pte *pgd;
+	u64 pgd_val;
+	int agaw;
+	u16 did;
+
+	if (!ecap_nest(iommu->ecap)) {
+		pr_err_ratelimited("%s: No nested translation support\n",
+				   iommu->name);
+		return -ENODEV;
+	}
+
+	pte = intel_pasid_get_entry(dev, pasid);
+	if (WARN_ON(!pte))
+		return -EINVAL;
+
+	/*
+	 * Caller must ensure PASID entry is not in use, i.e. not bind the
+	 * same PASID to the same device twice.
+	 */
+	if (pasid_pte_is_present(pte))
+		return -EBUSY;
+	pasid_clear_entry(pte);
+
+	/*
+	 * Sanity checking performed by caller to make sure address width
+	 * matching in two dimensions: CPU vs. IOMMU, guest vs. host.
+	 */
+	switch (s1_cfg->addr_width) {
+#ifdef CONFIG_X86
+	case ADDR_WIDTH_5LEVEL:
+		if (!cpu_feature_enabled(X86_FEATURE_LA57) ||
+		    !cap_5lp_support(iommu->cap)) {
+			dev_err_ratelimited(dev, "5-level paging not supported\n");
+			return -EINVAL;
+		}
+
+		pasid_set_flpm(pte, 1);
+		break;
+#endif
+	case ADDR_WIDTH_4LEVEL:
+		pasid_set_flpm(pte, 0);
+		break;
+	default:
+		dev_err_ratelimited(dev, "Invalid guest address width %d\n",
+				    s1_cfg->addr_width);
+		return -EINVAL;
+	}
+
+	/*
+	 * First level PGD is in GPA, must be supported by the second level.
+	 * Revisit: Is this sufficient?
+	 */
+	if ((uintptr_t)s1_gpgd > s2_domain->max_addr) {
+		dev_err_ratelimited(dev,
+				    "Guest PGD %lx not supported, max %llx\n",
+				    (uintptr_t)s1_gpgd, s2_domain->max_addr);
+		return -EINVAL;
+	}
+	pasid_set_flptr(pte, (uintptr_t)s1_gpgd);
+
+	if (s1_cfg->flags & IOMMU_VTD_PGTBL_SRE) {
+		if (!ecap_srs(iommu->ecap)) {
+			pr_err_ratelimited("No supervisor request support on %s\n",
+					   iommu->name);
+			return -EINVAL;
+		}
+		pasid_set_sre(pte);
+		/* Enable write protect WP if guest requested */
+		if (s1_cfg->flags & IOMMU_VTD_PGTBL_WPE)
+			pasid_set_wpe(pte);
+	}
+
+	if (s1_cfg->flags & IOMMU_VTD_PGTBL_EAFE) {
+		if (!ecap_eafs(iommu->ecap)) {
+			pr_err_ratelimited("No extended access flag support on %s\n",
+					   iommu->name);
+			return -EINVAL;
+		}
+		pasid_set_eafe(pte);
+	}
+
+	/*
+	 * Memory type is only applicable to devices inside processor coherent
+	 * domain. Will add MTS support once coherent devices are available.
+	 */
+	if (s1_cfg->flags & IOMMU_VTD_PGTBL_MTS_MASK) {
+		pr_warn_ratelimited("No memory type support %s\n",
+				    iommu->name);
+		return -EINVAL;
+	}
+
+	/* Setup the second level based on the given domain */
+	pgd = s2_domain->pgd;
+	agaw = iommu_skip_agaw(s2_domain, iommu, &pgd);
+	if (agaw < 0) {
+		dev_err_ratelimited(dev, "Invalid domain page table\n");
+		return -EINVAL;
+	}
+
+	pgd_val = virt_to_phys(pgd);
+	pasid_set_slptr(pte, pgd_val);
+	pasid_set_fault_enable(pte);
+
+	did = s2_domain->iommu_did[iommu->seq_id];
+	pasid_set_domain_id(pte, did);
+
+	pasid_set_address_width(pte, agaw);
+	pasid_set_page_snoop(pte, !!ecap_smpwc(iommu->ecap));
+
+	pasid_set_translation_type(pte, PASID_ENTRY_PGTT_NESTED);
+	pasid_set_present(pte);
+	pasid_flush_caches(iommu, pte, pasid, did);
+
+	return 0;
+}
diff --git a/drivers/iommu/intel/pasid.h b/drivers/iommu/intel/pasid.h
index ab4408c824a5..6bdc96b5de5d 100644
--- a/drivers/iommu/intel/pasid.h
+++ b/drivers/iommu/intel/pasid.h
@@ -118,6 +118,11 @@ int intel_pasid_setup_second_level(struct intel_iommu *iommu,
 int intel_pasid_setup_pass_through(struct intel_iommu *iommu,
 				   struct dmar_domain *domain,
 				   struct device *dev, u32 pasid);
+int intel_pasid_setup_nested(struct intel_iommu *iommu,
+			     struct device *dev,
+			     u32 pasid, pgd_t *s1_gpgd,
+			     struct dmar_domain *s2_domain,
+			     struct iommu_stage1_config_vtd *s1_cfg);
 void intel_pasid_tear_down_entry(struct intel_iommu *iommu,
 				 struct device *dev, u32 pasid,
 				 bool fault_ignore);
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 128be09fa7b6..f14f14f0f20e 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -619,6 +619,7 @@ struct subdev_domain_info {
 /* PCI domain-device relationship */
 struct device_domain_info {
 	struct list_head link;	/* link to domain siblings */
+	struct list_head nested; /* link to nested domain siblings */
 	struct list_head global; /* link to global list */
 	struct list_head table;	/* link to pasid table */
 	u32 segment;		/* PCI segment number */
-- 
2.34.1

